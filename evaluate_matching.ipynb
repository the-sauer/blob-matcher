{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluating 2025_11_05 with lambda=96\n",
      "fpr95=0.37279790293081366 for image 23 with 317 features\n",
      "fpr95=0.2711958022645678 for image 28 with 213 features\n",
      "fpr95=0.6423611111111112 for image 76 with 32 features\n",
      "fpr95=0.19751552795031055 for image 79 with 35 features\n",
      "fpr95=0.7236874038463568 for image 85 with 487 features\n",
      "fpr95=0.04630341587756349 for image 99 with 436 features\n",
      "fpr95=0.11487549754697769 for image 101 with 416 features\n",
      "fpr95=0.0704004264897524 for image 103 with 184 features\n",
      "fpr95=0.08747338735746213 for image 107 with 670 features\n",
      "fpr95=0.6636419778812529 for image 108 with 514 features\n",
      "avg_fpr95=0.3129410234611788\n",
      "\n",
      "# Evaluating 2025_11_05 with lambda=128\n",
      "fpr95=0.2331394740383034 for image 23 with 317 features\n",
      "fpr95=0.2283899475283071 for image 28 with 213 features\n",
      "fpr95=0.7599206349206349 for image 76 with 32 features\n",
      "fpr95=0.15610766045548655 for image 79 with 35 features\n",
      "fpr95=0.6429827097547541 for image 85 with 487 features\n",
      "fpr95=0.02893963492347718 for image 99 with 436 features\n",
      "fpr95=0.11153151902249375 for image 101 with 416 features\n",
      "fpr95=0.03580736879516645 for image 103 with 184 features\n",
      "fpr95=0.056756545874065076 for image 107 with 670 features\n",
      "fpr95=0.741428890766427 for image 108 with 514 features\n",
      "avg_fpr95=0.290648594592801\n",
      "\n",
      "# Evaluating 2025_11_06 with lambda=96\n",
      "fpr95=0.03637976487708125 for image 23 with 317 features\n",
      "fpr95=0.054007180336923505 for image 28 with 213 features\n",
      "fpr95=0.3080357142857143 for image 76 with 32 features\n",
      "fpr95=0.00041407867494824016 for image 79 with 35 features\n",
      "fpr95=0.31969121095027764 for image 85 with 487 features\n",
      "fpr95=0.008039372649806718 for image 99 with 436 features\n",
      "fpr95=0.027503934092381745 for image 101 with 416 features\n",
      "fpr95=0.004072384788532164 for image 103 with 184 features\n",
      "fpr95=0.007869539531617491 for image 107 with 670 features\n",
      "fpr95=0.16222687818018558 for image 108 with 514 features\n",
      "avg_fpr95=0.09031883216939876\n",
      "\n",
      "# Evaluating 2025_11_06 with lambda=128\n",
      "fpr95=0.04651128021887661 for image 23 with 317 features\n",
      "fpr95=0.019221209610604806 for image 28 with 213 features\n",
      "fpr95=0.6711309523809523 for image 76 with 32 features\n",
      "fpr95=0.00041407867494824016 for image 79 with 35 features\n",
      "fpr95=0.3496436643586275 for image 85 with 487 features\n",
      "fpr95=0.006780669693171404 for image 99 with 436 features\n",
      "fpr95=0.02592161899472369 for image 101 with 416 features\n",
      "fpr95=0.004872053074280298 for image 103 with 184 features\n",
      "fpr95=0.006267764983893081 for image 107 with 670 features\n",
      "fpr95=0.17405158009994734 for image 108 with 514 features\n",
      "avg_fpr95=0.09622667933208051\n"
     ]
    }
   ],
   "source": [
    "from modules.hardnet.eval_metrics import ErrorRateAt95Recall\n",
    "from modules.hardnet.losses import distance_matrix_vector\n",
    "from modules.hardnet.models import HardNet\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "image_descriptions = {\n",
    "    1: \"very oblique, close\",\n",
    "    3: \"very oblique, far\",\n",
    "    5: \"medium oblique, close\",\n",
    "    7: \"medium oblique, far\",\n",
    "    9: \"fronto-parallel, close\",\n",
    "    11: \"fronto-parallel, far\",\n",
    "    13: \"fronto-parallel, very far\",\n",
    "}\n",
    "\n",
    "for date in [\n",
    "       \"2025_11_05\",\n",
    "       \"2025_11_06\",\n",
    "    ]:\n",
    "    for scale in [96, 128]:\n",
    "        checkpoint = 199\n",
    "        print(f\"\\n# Evaluating {date} with lambda={scale}\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = HardNet(transform=\"PTN\", coords=\"log\", patch_size=32, scale=scale)\n",
    "        model.load_state_dict(torch.load(f\"./data/models/{date}_blobinator_{scale}/model_checkpoint_{checkpoint}.pth\", weights_only=False)[\"state_dict\"])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        positive_path_regex = re.compile(\"(\\\\d+)_(\\\\d+).png\")\n",
    "        dataset_path = \"./data/datasets/new/real/validation\"\n",
    "        patch_files = os.listdir(os.path.join(dataset_path, f\"patches/{scale}/positives\"))\n",
    "\n",
    "        overall_anchor_features = []\n",
    "        overall_positive_features = []\n",
    "        all_anchor_patches = []\n",
    "        all_positive_patches = []\n",
    "        overall_garbage_features = []\n",
    "        fpr95_sum = 0\n",
    "        fpr95_num = 0\n",
    "        for i in range(0, 123):\n",
    "            \n",
    "            regex = re.compile(f\"{i:04}_\\\\d+\\\\.png\")\n",
    "            patches = list(filter(lambda f: regex.match(f) is not None, patch_files))\n",
    "            if len(patches) == 0:\n",
    "                continue\n",
    "\n",
    "            anchor_patches = torch.empty((len(patches), 1, 32, 32)).to(device)\n",
    "            positive_patches = torch.empty((len(patches), 1, 32, 32)).to(device)\n",
    "\n",
    "            for j, patch_file in enumerate(patches):\n",
    "                match = positive_path_regex.search(os.path.basename(patch_file))\n",
    "                board_idx, blob_idx = match.group(1), match.group(2)\n",
    "                positive_patches[j] = torchvision.io.decode_image(os.path.join(dataset_path, f\"patches/{scale}/positives/{board_idx}_{blob_idx}.png\"), torchvision.io.ImageReadMode.GRAY).to(torch.float32) / 255\n",
    "                anchor_patches[j] = torchvision.io.decode_image(os.path.join(dataset_path, f\"patches/{scale}/anchors/{board_idx}_{blob_idx}.png\"), torchvision.io.ImageReadMode.GRAY).to(torch.float32) / 255\n",
    "            garbage_patch_files = os.listdir(os.path.join(dataset_path, f\"patches/{scale}/garbage\"))\n",
    "            garbage_patch_files = list(filter(lambda f: regex.match(f) is not None, garbage_patch_files))\n",
    "            garbage_patches = torch.empty((len(garbage_patch_files), 1, 32, 32)).to(device)\n",
    "            for j, patch_file in enumerate(garbage_patch_files):\n",
    "                garbage_patches[j] = torchvision.io.decode_image(os.path.join(dataset_path, f\"patches/{scale}/garbage/{patch_file}\"), torchvision.io.ImageReadMode.GRAY)\n",
    "\n",
    "            anchor_features, _ = model(anchor_patches)\n",
    "            positive_features, _ = model(positive_patches)\n",
    "            garbage_features, _ = model(garbage_patches)\n",
    "\n",
    "            distances = distance_matrix_vector(anchor_features, torch.concat((positive_features, garbage_features))).detach().cpu().numpy().flatten()\n",
    "            labels = torch.eye(anchor_features.size(0), positive_features.size(0) + garbage_features.size(0)).cpu().numpy().flatten()\n",
    "            fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n",
    "            print(f\"{fpr95=} for image {i} with {anchor_features.size(0)} features\")\n",
    "            fpr95_num += distances.size\n",
    "            fpr95_sum += distances.size * fpr95\n",
    "\n",
    "        avg_fpr95 = fpr95_sum / fpr95_num\n",
    "        print(f\"{avg_fpr95=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
